{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MCMC_DSfPG.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "1lhLeXQyVRx0",
        "2bpSqCegN83c",
        "xYUTnOOAmcs2",
        "48ncd8G-muwb",
        "SorNfTDinMjW",
        "2HEJukTj6n7-",
        "8nvqL49A-IU0"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vDBG9ZbgTjTA"
      },
      "source": [
        "# Data Science for Public Good: MCMC Hands On Example\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lhLeXQyVRx0"
      },
      "source": [
        "## Begin importing and installing necessary Python packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VTCLpxdgocKt"
      },
      "source": [
        "#Import important modules that we'll need to run this example\n",
        "  #numpy: One of the best and most useful science packages available in Python\n",
        "import numpy as np\n",
        "\n",
        "  #matplotlib: The most common graphing Python package\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import colors, colorbar\n",
        "\n",
        "  #Still matplotlib, but sub-packages needed for animations\n",
        "from matplotlib import rc\n",
        "rc('animation', html='jshtml')\n",
        "import matplotlib.animation as animation\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCOTflQ16u--"
      },
      "source": [
        "#These are some backend commands that you don't need to worry about\n",
        "  #This changes the font size on our plots\n",
        "plt.rcParams['font.size'] = 14\n",
        "\n",
        "  #This lets Google Colab make long videos\n",
        "plt.rcParams['animation.embed_limit'] = 2**128"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCXztep6sDMr"
      },
      "source": [
        "#Not all Python packages come loaded on Google Colab (or your local computer)! You will\n",
        "#most likely need to install them. The easiest way is by using pip (if you have a normal\n",
        "#Python installation) or 'conda install' (if you use anaconda).\n",
        "\n",
        "  #Google Colab uses pip as its package manager, so install the emcee and corner packages\n",
        "!pip install emcee corner\n",
        "\n",
        "  #NOTE: If you run this on your local machine, the above command will not work. You can use\n",
        "  #pip in a command line or conda install (depending on your Python installation) to install these\n",
        "  #packages\n",
        "  \n",
        "  #emcee: Python-based MCMC installation by Dan Foreman-Mackey\n",
        "import emcee\n",
        "\n",
        "  #corner: Makes corner plots of our parameters...more later!\n",
        "import corner"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b096uMVBWZY9"
      },
      "source": [
        "## A more in-depth example of MCMC for a linear data set we make (Not covered during the hands-on example)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tut20rJ6owlA"
      },
      "source": [
        "Let's begin by making a linear data set that we will use for this example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IgUG21KToqVA"
      },
      "source": [
        "#Choose values for the slope (m) and intercept (b) below\n",
        "m, b = 1, 2\n",
        "\n",
        "#Choose an (integer) number of data points to generate our data set\n",
        "N = 50\n",
        "\n",
        "#Randomly choose N numbers between 0 and 10 (exclusive)\n",
        "x_values = np.sort(10 * np.random.rand(N))\n",
        "\n",
        "#Use m and b above to construct our data set\n",
        "y_values = m * x_values + b"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-K58VIi8nMDw"
      },
      "source": [
        "Now, let's plot the data we just made:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TmyXJm1Xo9iy"
      },
      "source": [
        "plt.figure(figsize=(6, 6))\n",
        "plt.plot(x_values, y_values, 'ro')\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.title(\"Test Data\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82uQ6ZKJpQyT"
      },
      "source": [
        "Noise (or statistical error) is an issue that affects all quantitative data. All measurements will have some error that cannot (and will not) ever go away. For this example, let's add random (or Gaussian) noise to our data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3BkFZfbpHZg"
      },
      "source": [
        "#Choose N numbers from 0 to 1\n",
        "delta_y = np.random.rand(N)\n",
        "\n",
        "#Calculate the shift (or noise) in our \"measurement\" by adding Gaussian noise\n",
        "shift = delta_y * np.random.normal(0, 1, N)\n",
        "\n",
        "#Now actually shift the data\n",
        "y_values += shift"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6_tz7pvoPqy"
      },
      "source": [
        "Let's plot the data again, but, this time, the data have \"noise\" added to them:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yX3GS_KLph-u"
      },
      "source": [
        "plt.figure(figsize=(6, 6))\n",
        "plt.errorbar(x_values, y_values, delta_y, fmt='o', color='r')\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.legend()\n",
        "plt.title(\"Noisy Data\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYQFND1Mos65"
      },
      "source": [
        "Now, we have an issue. Originally, our data were nice and linear, and we could easily read off the slope and intercept. Now that we've added noise, it's not so easy to tell what values best fit the data. This is where model fitting comes in (e.g., linear regression). Here, however, we will use an MCMC process to fit the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mSpVOO8bNwQT"
      },
      "source": [
        "In the presentation, I gave a short intro into what MCMC. In the Mauna Loa example below, I go into much more detail about the (negative) log likelihood. We'll take the (negative) log likelihood function as described in the following example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Og2putPxpmy8"
      },
      "source": [
        "#The preferred model parameters occur where the negative (* 1/2) chi^2 is minimized\n",
        "def log_prob(params, x, y, yerr):\n",
        "  m, b = params\n",
        "  model = m * x + b\n",
        "  return -0.5 * np.sum((y - model)**2.0 / yerr**2.0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AP7UyRiqqa4o"
      },
      "source": [
        "Now, we need to set up emcee to run our MCMC fitting. To do this, we need to define the number of walkers and their initial starting positions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBVve11PylXK"
      },
      "source": [
        "#How many walkers do we want?\n",
        "nwalkers = 100\n",
        "\n",
        "#Where do you want to center the walkers in parameter space?\n",
        "m_start, b_start = -9, 10\n",
        "\n",
        "#Now determine the initial positions\n",
        "positions = np.array([m_start, b_start]) + np.random.randn(nwalkers, 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZAmsl5MrFpA"
      },
      "source": [
        "The last step is determining how many steps you want to take. I've tested this on Google Colab. For the animations (below) to work, this should be set to $<500$. Luckily this is an easy example, so that many steps (may) be overkill."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMys-0G5rFFZ"
      },
      "source": [
        "#Number of steps\n",
        "steps = 500\n",
        "\n",
        "#Make the sampler object\n",
        "sampler = emcee.EnsembleSampler(nwalkers, 2, log_prob, args=(x_values, y_values, delta_y));\n",
        "\n",
        "#And run it\n",
        "sampler.run_mcmc(positions, steps, progress=True);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xF1GjAZroDu"
      },
      "source": [
        "Now, we need to get the MCMC \"chain.\" This is a fancy way of getting the location of each walker in time. Anecdotally, this is the same as a walking(or running or biking) tracker that records your location on Earth in time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGcwEGjsztVA"
      },
      "source": [
        "chain = sampler.get_chain()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zc3hxH6ysB8m"
      },
      "source": [
        "Now, let's make the trace plots for one walker in time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rP_yruTzjLM"
      },
      "source": [
        "fig, ax = plt.subplots(1, 3, figsize=(12, 4))\n",
        "\n",
        "#Plot the first walker's slope position in time\n",
        "ax[0].plot(chain[:, 0, 0], 'b.-')\n",
        "ax[0].axhline(m, ls='--', color='k')\n",
        "\n",
        "#Plot the first walker's intercept position in time\n",
        "ax[1].plot(chain[:, 0, 1], 'b.-')\n",
        "ax[1].axhline(b, ls='--', color='k')\n",
        "\n",
        "#Plot the first walker's position in parameter space\n",
        "color = ax[2].scatter(chain[:, 0, 0], chain[:, 0, 1], c=np.arange(chain.shape[0]), cmap='jet')\n",
        "ax[2].axvline(m, ls='--', color='k')\n",
        "ax[2].axhline(b, ls='--', color='k')\n",
        "\n",
        "#The color bar shows the step number (bluer is early, red is late)\n",
        "cbar = plt.colorbar(color)\n",
        "cbar.set_label(\"Step Number\")\n",
        "\n",
        "#These set the x and y labels\n",
        "ax[0].set_xlabel(\"Step Number\")\n",
        "ax[1].set_xlabel(\"Step Number\")\n",
        "\n",
        "ax[0].set_ylabel(\"m\")\n",
        "ax[1].set_ylabel(\"b\")\n",
        "\n",
        "ax[2].set_xlabel(\"m\")\n",
        "ax[2].set_ylabel(\"b\")\n",
        "\n",
        "plt.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mDhGDT0ssAr"
      },
      "source": [
        "All of this code below is to make the animation of the first walker's position in time. Feel free to look at this. Note that if you used more than 1000 steps, this won't work (it will silently fail). This may take a few minutes, so sit back and relax..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWfA3fuw321e"
      },
      "source": [
        "fig_anim, ax_anim = plt.subplots(1, 4, figsize=(20, 4), gridspec_kw={'width_ratios': [15, 15, 15, 1]});\n",
        "\n",
        "m_line, = ax_anim[0].plot([], [], 'b.-', animated=True)\n",
        "b_line, = ax_anim[1].plot([], [], 'b.-', animated=True)\n",
        "mb_scatter = ax_anim[2].scatter([], [], animated=True)\n",
        "\n",
        "cmap = plt.get_cmap('jet')\n",
        "\n",
        "for i in range(2):\n",
        "  ax_anim[i].set_xlim(0, chain.shape[0] + 1)\n",
        "\n",
        "ax_anim[0].set_ylim(ax[0].get_ylim())\n",
        "ax_anim[1].set_ylim(ax[1].get_ylim())\n",
        "\n",
        "ax_anim[2].set_xlim(ax[2].get_xlim())\n",
        "ax_anim[2].set_ylim(ax[2].get_ylim())\n",
        "\n",
        "ax_anim[0].axhline(m, color='k', ls='--')\n",
        "ax_anim[1].axhline(b, color='k', ls='--')\n",
        "ax_anim[2].axvline(m, ls='--', color='k')\n",
        "ax_anim[2].axhline(b, ls='--', color='k')\n",
        "\n",
        "ax_anim[0].set_xlabel(\"Step Number\")\n",
        "ax_anim[1].set_xlabel(\"Step Number\")\n",
        "\n",
        "ax_anim[0].set_ylabel(\"m\")\n",
        "ax_anim[1].set_ylabel(\"b\")\n",
        "\n",
        "ax_anim[2].set_xlabel(\"m\")\n",
        "ax_anim[2].set_ylabel(\"b\")\n",
        "\n",
        "fig_anim.text(0.93, 0.5, \"Step Number\", rotation=90, va='center')\n",
        "plt.tight_layout()\n",
        "\n",
        "anim_lines = [m_line, b_line, mb_scatter]\n",
        "\n",
        "cbar =  colorbar.ColorbarBase(ax_anim[3], cmap=cmap, norm=colors.Normalize(0, chain.shape[0]), orientation='vertical')\n",
        "def frame(N):\n",
        "  anim_lines[0].set_data(np.arange(N), chain[:N, 0, 0])\n",
        "  anim_lines[1].set_data(np.arange(N), chain[:N, 0, 1])\n",
        "\n",
        "  time_steps = cmap(np.linspace(0, 1, chain.shape[0]))\n",
        "\n",
        "  anim_lines[2].set_offsets(np.c_[chain[:N, 0, 0], chain[:N, 0, 1]])\n",
        "  anim_lines[2].set_facecolors(time_steps)\n",
        "\n",
        "anim = animation.FuncAnimation(fig_anim, frame, frames=chain.shape[0], blit=False, repeat=True)\n",
        "anim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdZOewY2Ll6P"
      },
      "source": [
        "Pretty cool, huh? While this makes a nice visual representation of what MCMC is doing, in reality, we always use more than 1 walker. In this case, we should make a trace plot using all of the walkers on one plot: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLScZnjnJfCz"
      },
      "source": [
        "fig, ax = plt.subplots(1, 2, figsize=(12, 6), sharex=True)\n",
        "\n",
        "#Plot all of the walkers' slope positions\n",
        "ax[0].plot(chain[:, :, 0], 'k-', alpha=0.5);\n",
        "ax[0].axhline(m, color='r', ls='--')\n",
        "\n",
        "#Plot all of the walkers' intercept positions\n",
        "ax[1].plot(chain[:, :, 1], 'k-', alpha=0.5);\n",
        "ax[1].axhline(b, color='r', ls='--')\n",
        "\n",
        "#Add labels\n",
        "ax[0].set_xlabel(\"Step Number\")\n",
        "ax[1].set_xlabel(\"Step Number\")\n",
        "\n",
        "ax[0].set_ylabel(\"m\")\n",
        "ax[1].set_ylabel(\"b\");\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzhMxbp6u47H"
      },
      "source": [
        "What we didn't see in the one walker trace plots is this \"fuzzy\" or widened trace that shrinks at high step numbers. This is a key indication that our fit has truly \"converged.\" \n",
        "\n",
        "The \"fuzzy\" portion of this plot is where the walkers are generally traversing the parameter space but haven't found a valley yet. We only care about the parameter space where the walkers generally have started to find this valley. The time, or number of steps, it takes for this to occur is known as the \"burn length.\" If we make it too high (relative to the number of steps), we won't have enough data points to sample the parameter space. If we make it too low, we're sampling parts of the parameter space that don't describe our data.\n",
        "\n",
        "For this example, I'll choose a burn length of 200 steps. However, I highly suggest changing this number (below) and seeing how it effects the corner plots."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OeVsj9RxwaaV"
      },
      "source": [
        "#Choose a burn length\n",
        "burn_length = 200\n",
        "\n",
        "#Get the burned chain\n",
        "burned_chain = sampler.get_chain(discard=burn_length, thin=1, flat=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYpfZrfwwcZG"
      },
      "source": [
        "Now, we get to make the all important corner plot that shows the histograms of our fitted parameters (known as posterior distributions) and the parameter space of the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGSNN34iLwEV"
      },
      "source": [
        "fig = corner.corner(burned_chain, labels=[\"m\", \"b\"], truths=[m, b], quantiles=[0.16, 0.5, 0.84], show_titles=True, title_fmt=\".3f\")\n",
        "\n",
        "fig.text(0.70, 0.75, \"Posterior Distributions\", fontsize=16)\n",
        "fig.text(0.55, 0.725, \"←\", fontsize=60)\n",
        "fig.text(0.65, 0.62, \" ↓\", fontsize=60)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LAXoU49Ew7Fg"
      },
      "source": [
        "Not bad...notice that the \"truths\" (blue lines) don't exactly match the parameters you set above. This is due to the statistical noise that we added. Try going back to the top of this example and changing the \"size\" of this noise to see how it affects the results.\n",
        "\n",
        "The lower left panel shows the actual parameter space of the fit as sampled by the walkers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SPR9_WWyP8Y"
      },
      "source": [
        "### One last thing about MCMC (at least here, anyway...)\n",
        "\n",
        "In a least squares regression, we can use \"constraints\" to tell the fitting algorithm where we know (AKA a priori) the parameters cannot be. For example, even in the noisy data set, we know if the slope is either positive or negative. We can specifically tell (most) least squares algorithms where not to look.\n",
        "\n",
        "We can do the same thing in MCMC, but these constraints (known as priors) are more powerful. Since everything in MCMC fitting is a distribution, we can tell the program exactly where to (and where not to) look. For example, let's say we know that the slope is positive. In emcee, we can write the prior function as:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMVwd00ACZ4z"
      },
      "source": [
        "#Our new prior\n",
        "def log_prior_new(params):\n",
        "  m, b = params\n",
        "  if m >= 0:\n",
        "    return 0.0\n",
        "  else:\n",
        "    return -np.inf\n",
        "\n",
        "#Our new likelihood function with priors taken into account\n",
        "def log_prob_new(params, x, y, yerr, prior):\n",
        "  m, b = params\n",
        "  model = m * x + b\n",
        "  return -0.5 * np.sum((y - model)**2.0 / yerr**2.0) + prior(params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Twgimf3FEmqF"
      },
      "source": [
        "Notice that we return $0$ or $-\\infty$ from our prior function. $0$ is returned when the slope is $>0$ (which we want), and this does not affect our (negative) log likelihood. When the slope is outside of this constraint, it returns $-\\infty$ and tells the MCMC program to pick a different set of parameters.\n",
        "\n",
        "How does this affect our results?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dn3mNiBuEiAv"
      },
      "source": [
        "#Where do you want to center the walkers in parameter space?\n",
        "  #Note that the starting values you choose MUST be where the prior is valid otherwise it won't work properly\n",
        "m_start, b_start = 9, 10\n",
        "\n",
        "#Now determine the initial positions\n",
        "positions = np.array([m_start, b_start]) + np.random.randn(nwalkers, 2)\n",
        "\n",
        "#Run MCMC again\n",
        "sampler_prior = emcee.EnsembleSampler(100, 2, log_prob_new, args=(x_values, y_values, delta_y, log_prior_new));\n",
        "sampler_prior.run_mcmc(positions, steps, progress=True);\n",
        "\n",
        "chain = sampler_prior.get_chain()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "feaEIg8VHkOw"
      },
      "source": [
        "fig, ax = plt.subplots(1, 3, figsize=(12, 4))\n",
        "ax[0].plot(chain[:, 0, 0], 'b.-')\n",
        "ax[0].axhline(m, ls='--', color='k')\n",
        "\n",
        "ax[1].plot(chain[:, 0, 1], 'b.-')\n",
        "ax[1].axhline(b, ls='--', color='k')\n",
        "\n",
        "color = ax[2].scatter(chain[:, 0, 0], chain[:, 0, 1], c=np.arange(chain.shape[0]), cmap='jet')\n",
        "ax[2].axvline(m, ls='--', color='k')\n",
        "ax[2].axhline(b, ls='--', color='k')\n",
        "\n",
        "cbar = plt.colorbar(color)\n",
        "cbar.set_label(\"Step Number\")\n",
        "\n",
        "ax[0].set_xlabel(\"Step Number\")\n",
        "ax[1].set_xlabel(\"Step Number\")\n",
        "\n",
        "ax[0].set_ylabel(\"m\")\n",
        "ax[1].set_ylabel(\"b\")\n",
        "\n",
        "ax[2].set_xlabel(\"m\")\n",
        "ax[2].set_ylabel(\"b\")\n",
        "\n",
        "plt.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3jUJXvwHntb"
      },
      "source": [
        "fig, ax = plt.subplots(1, 2, figsize=(12, 6), sharex=True)\n",
        "ax[0].plot(chain[:, :, 0], 'k-', alpha=0.5);\n",
        "ax[0].axhline(m, color='r', ls='--')\n",
        "\n",
        "ax[1].plot(chain[:, :, 1], 'k-', alpha=0.5);\n",
        "ax[1].axhline(b, color='r', ls='--')\n",
        "\n",
        "ax[0].set_xlabel(\"Step Number\")\n",
        "ax[1].set_xlabel(\"Step Number\")\n",
        "\n",
        "ax[0].set_ylabel(\"m\")\n",
        "ax[1].set_ylabel(\"b\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-25ZNLtfHr-k"
      },
      "source": [
        "burned_chain = sampler.get_chain(discard=burn_length, thin=1, flat=True)\n",
        "\n",
        "fig = corner.corner(burned_chain, labels=[\"m\", \"b\"], truths=[m, b], quantiles=[0.16, 0.5, 0.84], show_titles=True, title_fmt=\".3f\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tIvorka01ne9"
      },
      "source": [
        "Did anything really change in this example? No, but that's because a linear regression is very easily fit using MCMC. For more complicated models/data, the use of priors can be extremely important."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COxWdvVCJLAL"
      },
      "source": [
        "**Question: Why do we need priors at all?**\n",
        "\n",
        "For this toy model, we know certain properties of the data already (i.e., the slope and y-intercept must both be greater than 0). Since negative values for either parameter won't fit the data well, we don't have to probe those ranges at all. \n",
        "\n",
        "In everyday science use, however, priors can be used in several ways:\n",
        "\n",
        "\n",
        "1.   Removing unphysical values altogether (i.e., $T < 0~\\text{Kelvin}$)\n",
        "2.   Using previously determined parameters from different studies (i.e., someone previously calculating an exoplanet's mass of $1.2\\pm0.1$ Jupiter masses)\n",
        "3.  Best guesses about what the parameter values might be (i.e., \"the slope looks like it's between 2 and 20\")\n",
        "\n",
        "In cases 2 & 3, adding more data to your fits will reduce the effect that your priors have on your posterior distribution. \n",
        "\n",
        "*Overall, we use prior distributions to reflect what we already know about the data/system.*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bpSqCegN83c"
      },
      "source": [
        "## Mauna Loa CO$_2$ Data Set: Hands-On"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSnnvrRJICxM"
      },
      "source": [
        "#Let's load in the CO2 data set...\n",
        "time, co2_ppm, co2_err = np.loadtxt(\"co2_weekly_mlo.csv\", usecols=(3, 4, 5), unpack=True, skiprows=1, delimiter=',')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRbA_GljPSjm"
      },
      "source": [
        "#...and plot it!\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.plot(time, co2_ppm)\n",
        "plt.xlabel(\"Year\")\n",
        "plt.ylabel(\"CO$_2$ concentration [ppm]\")\n",
        "plt.title(\"Mauna Loa Monitoring Station: Uncleaned\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMLFu_eEXP_Y"
      },
      "source": [
        "Uh oh...there are some points where the data are missing and have been filled with -1000. Let's remove those first:\n",
        "\n",
        "(see more about data cleaning in Alex's activity)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ExeKFVhCPYGZ"
      },
      "source": [
        "#Find where the concentration data is < 0 (where they are unphysical)...\n",
        "unphys_mask = np.less(co2_ppm, 0)\n",
        "\n",
        "#...and delete them from the time and concentration arrays.\n",
        "time = np.delete(time, unphys_mask)\n",
        "co2_ppm = np.delete(co2_ppm, unphys_mask)\n",
        "co2_err = np.delete(co2_err, unphys_mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bY8IJthDQiDj"
      },
      "source": [
        "#Now, plot the fixed data:\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.plot(time, co2_ppm)\n",
        "plt.xlabel(\"Year\")\n",
        "plt.ylabel(\"CO$_2$ concentration [ppm]\")\n",
        "plt.title(\"Mauna Loa Monitoring Station: Cleaned\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zyRLVpGYPPu"
      },
      "source": [
        "Much better. The next thing we need to decide on is what kind of function we'll use to fit the data. There are two distinct features of the data:\n",
        "\n",
        "\n",
        "1.   An oscillating (or periodic) modulation\n",
        "2.   A base function that is increasing faster than a linear function\n",
        "\n",
        "So, I suggest we use:\n",
        "$$\\text{CO}_2(t) = at^2+bt+\\text{amp}\\cdot\\cos(2\\pi t+\\phi)+c$$ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYUTnOOAmcs2"
      },
      "source": [
        "#### Question: How many parameters do we have?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24dXmHUba_w9"
      },
      "source": [
        "##### Answer\n",
        "\n",
        " 5 (a, b, c, amp, $\\phi$)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVo0jzPebUgm"
      },
      "source": [
        "#### Log Likelihood function\n",
        "\n",
        "In linear least squares, we minimize the summed, squared residual. In MCMC, we minimize a function called the (negative) log likelihood:\n",
        "\n",
        "$$\\mathcal{L}=-\\dfrac{1}{2}\\sum\\left(\\dfrac{(\\text{data} - \\text{model})^2}{\\text{error}^2}\\right)$$\n",
        "\n",
        "This is similar to the summed, squared residual, but there are two changes:\n",
        "\n",
        "\n",
        "1.   There's a $-\\frac{1}{2}$ in front of the sum\n",
        "2.   We divide the squared residual by the error/uncertainty of our data\n",
        "\n",
        "(This is commonly known as  $-\\frac{1}{2}\\chi^2$)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48ncd8G-muwb"
      },
      "source": [
        "#### Question: Why do we need to divide by the error?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUPm8NaRdqOL"
      },
      "source": [
        "##### Answer: $\\mathcal{L}$ needs to be unitless, while the summed, squared residual can be have a unit. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0mxtcqzdqmW"
      },
      "source": [
        " #### Let's write the log likelihood function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovYtFcaBYRE7"
      },
      "source": [
        "#Calculate the (negative) log likelihood\n",
        "def log_prob(params, t, y, yerr, prior_func):\n",
        "\n",
        "  #This line shifts the time relative to January 1, 1970\n",
        "  #This is NOT necessary at all. It makes the calculation\n",
        "  #finish/converge quicker\n",
        "  t0 = t - 1970.\n",
        "\n",
        "  #These 5 parameters are chosen using a random walk\n",
        "  a, b, c, amp, phase= params\n",
        "\n",
        "  #Calculate the model function using these 5 parameters\n",
        "  model = a * t0**2.0 + b * t0 + c + amp * np.cos(2.0 * np.pi * t0 + phase)\n",
        "  \n",
        "  #Calculate L and return the value\n",
        "  return -0.5 * np.sum(((y - model)/ yerr)**2.0) + prior_func(params)\n",
        "\n",
        "  #This is a prior function (which I don't have time to go over)! \n",
        "  #See the more in-depth example above\n",
        "def log_prior(params):\n",
        "  a, b, c, amp, phase = params\n",
        "\n",
        "  #Only allow each data point to have a phase between 0 and 2 pi\n",
        "  #The phase is degenerate above this\n",
        "  if phase >= 0 and phase <= 2 * np.pi:\n",
        "    return 0.0\n",
        "  return -np.inf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31naVMIMnZ10"
      },
      "source": [
        "#### Now, we have to the parameters we need to define in order to use emcee"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jlqVlazOnJau"
      },
      "source": [
        "#ndim is the number of parameters (dimensions) we're fitting\n",
        "ndim = 5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SorNfTDinMjW"
      },
      "source": [
        "#### Question: Is there such a thing as \"too many\" or \"too few\" walkers?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7ntRAy3oAjc"
      },
      "source": [
        "##### Answer: Yes. If you add too many walkers, the program will run very slowly. If you don't add enough, you will need to run more steps to get a good sample. There is no defined or accepted way to determine how many to use."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2wI4HACpnPT"
      },
      "source": [
        "### Define the number of walkers and determine their random starting points"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cjPYTgHarui"
      },
      "source": [
        "  #nwalkers is the number of MCMC walkers we want\n",
        "nwalkers = 500\n",
        "\n",
        "  #Randomly sample the parameter space around a = 0, b = 1, c = 300, amp = 5, and phi = 0\n",
        "  #NB: I fine tuned these values so that the MCMC algorithm would converge very quickly\n",
        "  #Suggestion: Change these parameters (after we send this notebook out) to see how\n",
        "  #the initial sampling point changes the number of steps required for convergence\n",
        "positions = np.array([0, 1, 300, 5, 0]) + np.random.randn(nwalkers, ndim)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5hzWLIar1je"
      },
      "source": [
        "### Feed emcee all the parameters/functions we've defined and run it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1JVaUxKr9-s"
      },
      "source": [
        "#How many steps should we run?\n",
        "steps = 2000\n",
        "\n",
        "#Create the sampling object...\n",
        "sampler = emcee.EnsembleSampler(nwalkers, ndim, log_prob, args=(time, co2_ppm, co2_err, log_prior));\n",
        "\n",
        "#...and run MCMC!\n",
        "sampler.run_mcmc(positions, steps, progress=True);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iiITzfipt4lJ"
      },
      "source": [
        "### Trace plots"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPyosS-Xa98N"
      },
      "source": [
        "#In MCMC, a \"chain\" is a record of each walker's steps in time.\n",
        "#Visualizing the chain is a good way to determine if the MCMC\n",
        "#has \"converged\"\n",
        "chain = sampler.get_chain()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cUeqIErbSzo"
      },
      "source": [
        "#Plotting the traces for all walkers using all steps\n",
        "fig, ax = plt.subplots(ndim, 1, figsize=(12, 12), sharex=True)\n",
        "labels = [\"a [ppm/yr$^2$]\", \"b [ppm/yr]\", \"c [ppm]\", \"amp. [ppm]\", \"$\\\\phi$ [rad]\"]\n",
        "\n",
        "for i in range(ndim):\n",
        "  ax[i].plot(chain[:, :, i], 'k-', alpha=0.5);\n",
        "  ax[i].set_ylabel(labels[i])\n",
        "\n",
        "ax[-1].set_xlabel(\"Step Number\")\n",
        "plt.subplots_adjust(hspace=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42FLzqv_uVpf"
      },
      "source": [
        "Trace plots are a good way to determine if an MCMC run has converged. In the first few steps, the plots are \"fuzzy\", but they eventually become compact. The rough time where this occurs is known as the \"burn length.\"\n",
        "\n",
        "Question: Where (approximately) should we choose the burn length for these plots?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ss8r4Lmp8-FZ"
      },
      "source": [
        "burn_length = 300"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2HEJukTj6n7-"
      },
      "source": [
        "### More advanced topic: How do we determine if a chain has converged?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1H8DieK6wm1"
      },
      "source": [
        "#### The most common way is by using the \"autocorrelation\" time. In not-so-technical terms, this is number of steps required for a walker to \"forget\" where it was. emcee has a nice set of tutorials (with math) to explain this. \n",
        "\n",
        "https://emcee.readthedocs.io/en/stable/tutorials/autocorr/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxSb-9rz6iCT"
      },
      "source": [
        "### Corner Plots\n",
        "\n",
        "One of (if not the most) useful MCMC plots is known as the corner plot. On the diagonal are the histograms (known as posterior distributions) of the 5 fitted parameters. The other panels show the locations of the walkers for each pair of parameters. This is similar to the \"parameter space\" plots we saw in the presentation, but in more dimensions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V96nE3PxdFGa"
      },
      "source": [
        "burned_chain = sampler.get_chain(discard=burn_length, thin=1, flat=True)\n",
        "\n",
        "fig = corner.corner(burned_chain, quantiles=[0.16, 0.5, 0.84], show_titles=True, title_fmt=\".3f\", titles=labels, title_kwargs={'fontsize' : 11.5})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFnDbHXk9zBx"
      },
      "source": [
        "This data set shows the amplitude and $\\phi$ have a double valley parameter space! We would not have known/seen this if we used least squares regression."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8nvqL49A-IU0"
      },
      "source": [
        "### Question: Can you think of the reason why there are two valleys in the amplitude-$\\phi$ parameter space?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Y5XtdLi-gMS"
      },
      "source": [
        "#### Answer: The phase shift, $\\phi$, has two peaks: $\\approx1.57, 4.71$. These are close to the value required to change $\\cos x$ into $\\sin x$ terms (i.e., $\\cos(x+1.57) \\approx -\\sin x$ and $\\cos(x+4.71)\\approx\\sin x$). Simply, this tells us that we should have used $\\sin$ instead of $\\cos$ in our function."
      ]
    }
  ]
}